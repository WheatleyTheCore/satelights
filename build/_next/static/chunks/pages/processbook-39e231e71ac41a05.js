(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[872],{1102:(e,t,o)=>{"use strict";o.r(t),o.d(t,{default:()=>r});var a=o(7876);o(4404);var i=o(3122),s=o(4587),n=o.n(s);let r=()=>(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.A,{}),(0,a.jsxs)("div",{style:{minWidth:"100vw",display:"flex",flexDirection:"column",alignItems:"center",paddingBottom:"40px"},children:[(0,a.jsxs)("div",{style:{maxWidth:"1000px",display:"flex",flexDirection:"column",alignItems:"center",justifyContent:"center",gap:"10px"},children:[(0,a.jsx)("h1",{children:"Process Book!"}),(0,a.jsx)("h2",{children:"Overview and motivation"}),(0,a.jsxs)("p",{children:["Around december of 2024 I got the urge to build something big. It had been a while since I'd made anything for fun, and I decided I wanted to put an LED matrix on my living room ceiling as a fun alternate light source for the room. Then, as I was remembering a tour of JPL I had been on the previous summer I remembered how taken I was with Daniel Brusby's"," ",(0,a.jsx)("a",{style:{display:"inline"},href:"https://danielbusby.com/pulse/",children:"pulse"}),"sculpture, which showed communication data between a particular space probe and its ground station. Thus, I decided I wanted my lights to be space-themed too, and so I decided I wanted them to show satellite data (hence the name).",(0,a.jsx)("iframe",{width:"1140",height:"658",src:"https://www.youtube.com/embed/LjLqWoWXRQc",title:"Mini Pulse Testing",frameborder:"0",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:!0})]}),(0,a.jsxs)("p",{children:["So I learned about propegating sattelite orbits and about keplerian elements and everything else I needed from Alfonso Gonzalez's incredible"," ",(0,a.jsx)("a",{href:"https://www.youtube.com/@alfonsogonzalez-astrodynam2207",children:"youtube channel"})," ","and then started to work. Following his lead I worked in matlab, and got a 3d model and animated matrix (to show to 2D projection that would go on the ceiling) graph working."]}),(0,a.jsx)(n(),{src:"/matlab_attempt.png",width:800,height:400}),(0,a.jsx)("p",{children:"That's as far as I got before getting super busy again, and I didn't work on it for months. But then I started my Data Visualization course at WPI and I could maybe do it for credit, which I thought was a great opportunity to actually get something done."}),(0,a.jsx)("p",{children:"When starting my second attempt, I realized there were a few main problems to solve:"}),(0,a.jsxs)("ol",{children:[(0,a.jsx)("li",{children:"I had to somehow keep a model of all the stuff I wanted to display"}),(0,a.jsx)("li",{children:"I had to do some evil camera math to get the picture to put on the LEDs"}),(0,a.jsx)("li",{children:"I had to somehow turn this picture into data for the LEDs"}),(0,a.jsx)("li",{children:"I had to have some sort of hardware to actually control the LEDs"})]}),(0,a.jsx)("p",{children:"At first I wanted to try to get this working on a single ESP32 with only satellite data becuase I love putting cool stuff on tiny systems, but time constraints put that notion out of my head."}),(0,a.jsx)("p",{children:"So then I figured I'd just start out by getting something working on my computer. Not having much graphics experience I figured the easiest thing to do would be to model stuff in NumPy, and then figure out the camera projections later. Then, once I started actually looking into camera projection math, I decided it would definitely be faster to find a system that handles that for me."}),(0,a.jsxs)("p",{children:["So then I started working with React Three Fiber, and got a simple visualization of stars working modeled after"," ",(0,a.jsxs)("a",{href:"https://www.math.uwaterloo.ca/tsp/star/hyg_tour.html",children:[" ","Waterloo University's HYG109399 Project"]}),". This was my first introduction to the HYG dataset, which was the star dataset I used in this project. There are a few other options, but this one is a combination of most of the larger other options. I couldn't find source code for Waterloo's tour examples, but by reading the page source I was able to figure out how it worked and recreate it (with a slightly different star dataset, as I filtered mine by magnitude instead of distance so that only visible stars would be included)."]}),(0,a.jsx)("video",{src:"simplestars.mp4",width:"800",height:"800",controls:"controls"}),(0,a.jsxs)("p",{children:["So then I started looking into a million types of graphics software, with a requirement that it had to be in python since I wanted to go fast (and my C/C++ for non-embedded systems is not as strong). The main things I looked into were"," ",(0,a.jsx)("a",{href:"https://www.pygame.org/news",children:"PyGame"})," (which doesn't support 3D by default),"," ",(0,a.jsx)("a",{href:"https://pyopengl.sourceforge.net/",children:"PyOpenGl"})," ","(which paired with pygame can do 3D),",(0,a.jsx)("a",{href:"https://www.panda3d.org/",children:"Panda3D"})," which I thought seemed like too much to learn with too little documentation, and"," ",(0,a.jsx)("a",{href:"https://www.ursinaengine.org/",children:"Ursina"})," , which seemed very similar to Threejs, and was the one I went with. (I'd later learn that Ursina is build on Panda3D and actually has worse documentation. I had to do a lot of hacking around the engine's limitations, and so I ended up learning Panda3D anyways coincedentally.) I nixed matplotlib as it didn't handle animation as well."]}),(0,a.jsxs)("p",{children:["Concurrently, as I was figuring out which 3D library to use, I was also desperately trying to figure out how I'd get images out of the program. Most of these libraries (with the exception of PyOpenGL) handle pixel matrix stuff for you, and don't offer many great ways to get the image written back out to the program for further processing. I considered writing a shader, but it needed to be something running on the CPU since it had to also communicate with the lights. I tried a number of libraries like"," ",(0,a.jsx)("a",{href:"https://pypi.org/project/selenium/",children:"PySelenium"})," ","or"," ",(0,a.jsx)("a",{href:"https://github.com/asweigart/pyautogui",children:"PyAutoGUI"})," ",'that "drive" the computer for you and let you take screenshots, but none could get any faster than 15 fps on my laptop, which was way too slow. I didn\'t figure out a solution to this problem until later.']}),(0,a.jsx)("p",{children:"I then tackled wiring the LED strings and setting up the hangers for them. I put 12 rows of plastic sticky hooks on the ceiling, as seen below, to hang the lights and power wires on."}),(0,a.jsx)(n(),{src:"/hooks.jpg",width:800,height:400}),(0,a.jsx)("p",{children:" These hooks took two entire evenings to put up."}),(0,a.jsx)("p",{children:"Then, since 1) I wanted to try a smaller task with the same type of lights and 2) my kitchen has horrible lighting, I put together some touch-activated under-cabinet lighting. This got me familiar with the WS2812B LED chipset, and helped me put together a known-good test rig to test the rest of the lights on. They are controlled by a little strip of copper tape stuck to the bottom of the cabinets. Fun fact, the electronics are housed in the tea box taped to the bottom of one of the cabinets."}),(0,a.jsx)("video",{src:"cabinets.MOV",width:"800",height:"800",controls:"controls"}),(0,a.jsx)("p",{children:"Then I went through the arduous process of wiring the LED matrix. I did this by first setting up a test rig on an old Raspberry Pi. Then I took my three 20-meter lengths of led strings (purchased online), and cut them into 12 16-ft segments. I then went through and tested them, marking which side I found to be data-in. Having previously calculatted the maximum wattage the system could draw in normal functioning, I got 14-gauge wire to do current injection at each end of each row. I set this power cable up in two lines, and then stripped small segments of it every foot (since the rows of lights were a foot apart). I then soldered all the power connections, and chained the data lines together so that the data-out of one string went to the data-in of the next. (Note: due to the nature of these strings, without some evil long-range data transmission cables and attenuation prevention, the direction of the lights is in a zig zag pattern, which has to be accounted for when creating the display code.)"}),(0,a.jsx)("p",{children:'After each added string of lights I tested a little "flash red" program I had to make sure data was making it thorugh the system okay and that there weren\'t any issues. Below is a photo of that. The soldering process was probably the longest part of the process'}),(0,a.jsx)(n(),{src:"/soldering.png",width:600,height:800}),(0,a.jsx)(n(),{src:"/power_supply.png",width:800,height:600}),(0,a.jsx)("p",{children:"Above is the 40W power supply that drives the system, powering the lights through the two large wires coming out (there are two since one may be insufficent at high current draws), a raspberry pi, and a little circuit that does logic-level shifting from the Pi's 3.3V to the LED strip's 5V. This may actually be unnecessary, but at one point I couldn't get one of the strings to light and I thought signal attenuation could be the issue. Unfortuantely, it was actually just that there was some sort of internal short that killed it. (As such, it became a 12 by 98 pixel display to an 11 by 98 pixel display.)"}),(0,a.jsx)("p",{children:"Once I had the soldering done, I hung the lights up and tested them. Surprisingly, they worked without much issue."}),(0,a.jsx)(n(),{src:"/lightstest.png",width:800,height:600}),(0,a.jsxs)("p",{children:["Then, I continued work on the ursina program. I write the stars into the system as single entity with a vertex for each star, and a little cube at each vertex. Functionally, it's the same as a particle system. This keeps it relatively efficient to put in, however due to the number of verticies I opted not to make them move as that would be computationally expensive, and at the time I still wanted to run this whole system on a Raspberry Pi. The sattelites were put in using the"," ",(0,a.jsx)("a",{href:"https://rhodesmill.org/pyephem/quick.html",children:"PyEphem"})," ","library. During the implementation of the planets I realized there was a more updated library called"," ",(0,a.jsx)("a",{href:"https://rhodesmill.org/skyfield/",children:"Skyfield"}),", which I then went back and converted some of the previous things I implemented to."," "]}),(0,a.jsx)(n(),{src:"/ursina.png",width:800,height:600}),(0,a.jsx)("p",{children:"Here, I have the pink sattelites, purple planets, two lines showing north and 0 degrees lat and lon. Based on these I could figure out what math needed to be done to place someone at WPI (orange line), and also convert polar coordinates to rectangular ones in Ursina (which is difficult because it uses coordinates that are in a different order and some (but only some) are flipped compared to the math world). This was a cause of great pain. You may also notice that there are not as many satellites in the sky as you'd expect. This is because many of the TLE elements from the NORAD API I was using were pretty old (aside from popular satellites like the ISS) and PyEphem requires them to be within a week or so. This is another thing I plan to go back and fix."}),(0,a.jsx)("p",{children:"I then went on a slight diversion since I wanted to add more interactivity, making web and mobile control interfaces I call satelSite and sAppelite. I won't go too in-depth with the details but to say the website controller code reused a lot of the graphics code from my orignial Three attempt, and the other one was built with React Native and loaded onto my phone using Expo"}),(0,a.jsx)("video",{src:"app.mp4",width:"800",height:"800",controls:"controls"}),(0,a.jsx)("video",{src:"site.MOV",width:"800",height:"800",controls:"controls"}),(0,a.jsxs)("p",{children:["I added a thread in the main simulation code for a websocket server to get directional data from peripherals. I also tried using shared memory for faster performance, but unfortunately there was too much overhead to get that working in a reasonable amount of time. You can see this working by the direction data moving the orange line (which is the observer, and is simply replaced with a camera object in the second video) and the camera in the second video. The app uses your phone's IMU data to simulate stargazing apps, and the site just uses drag controls. I had tried to get mobile IMU data working for the web interface too, and was able to get accelerometer data through the"," ",(0,a.jsx)("a",{href:"https://developer.mozilla.org/en-US/docs/Web/API/Accelerometer",children:"experiemental web sensors api"}),", but couldn't get magnetometer data, which is needed to properly orient the directional data from the accelerometer."]}),(0,a.jsx)("p",{children:"And then came the evil of getting image data back from the GPU/display and into the CPU's domain. This..... was absolutely horrible. Through a sacrifice of a million hours and a portion of my sanity (and reading thorugh a lot of the Panda3D docs and Ursina source code) I was able to learn that you can take a screenshot into a texture object and then get the data from the RAM from that object and then you have to do some incomprehensible wizardry to in order to get it into a recognizable shape. Once I had that working, the colorspace was mixed up and the image was flipped vertically, which were still a bit of a pain to figure out since it meant trying permutations of the RGB channels. (The alpha channel was also in the wrong place for anything else I know about to use it, but that was relatively easy to figure out)."}),(0,a.jsx)("p",{children:"Once that was working, I had to get the alorithm to turn the image into a stream of rgb data for the leds, with each 98-pixel segement being flipped since they are wired in the aformentioned zigzag pattern. This was relatively simple, however. In short, I just iterate over columns of cells, figure out what the nearest object in said cell is, and make that cell's corresponding matrix pixel's color the color of said nearest object. Sort of like convolution, but without the overlapping and in column-first direction, but also using a flipped version of the picture every other column."}),(0,a.jsx)("p",{children:"Then, I tried putting it all on the Pi and running it. This required messing with some python permissions in order to get it to play nicely with the led driver library (which requires root privelages), but otherwuise worked first try. Alas, the concurrent web socket thread and postprocessing math for conversion to led-strip data slowed it down too much. I believe this is solvable, but not within the time of this project."}),(0,a.jsxs)("p",{children:['So I pivoted to a system of three entities. There is the directional data from some external sensor (e.g. the app) which defaults to "up" if there\'s no websocket data, there\'s the "big compute" entity that runs the model, serves the sensor websocket server, and does the postprocessing math, and then the Raspberry Pi is just a display controller, which runs a websocket server and just writes whatever data it gets to the lights (within reason). Unfortuantely this was ',(0,a.jsx)("i",{children:"still"})," too much  for the pi, and I suspect there's some quirk about multithreading on it I have yet to find. In any case it now performs at a more accepable speed. In fact, in non-interactive settings where the only motion is the motion of satellites and planets across the sky, it will be more than fast enough, so it satisfies my original design requirements."]}),(0,a.jsx)("video",{src:"matrtix.mp4",width:"800",height:"800",controls:"controls"})]}),(0,a.jsx)("p",{children:"Above shows the lights being controlled with the mouse control on the simulation."}),(0,a.jsx)("h3",{children:"To Dos"}),(0,a.jsxs)("ol",{children:[(0,a.jsx)("li",{children:"Make stars move now that we've landed on a more computationally intensive simulation model. Not all of them have to update, but the sun at least should."}),(0,a.jsx)("li",{children:"Add moons to planets."}),(0,a.jsx)("li",{children:"Get Pi to render faster."}),(0,a.jsx)("li",{children:"Maybe actually write a shader for the image-to-matrix math."}),(0,a.jsx)("li",{children:"Make app and website controllers more robust."}),(0,a.jsx)("li",{children:"Add low-pass filtering to mobile IMU measurements to smooth shakiness."}),(0,a.jsx)("li",{children:"Cannibalize an old laptop or something to serve as a dedicated simulation server."})]})]})]})},3122:(e,t,o)=>{"use strict";o.d(t,{A:()=>i});var a=o(7876);o(4404);let i=()=>(0,a.jsxs)("nav",{class:"navbar navbar-expand-lg navbar-light bg-light",children:[(0,a.jsx)("a",{class:"navbar-brand",href:"/",children:"SatelLights"}),(0,a.jsx)("button",{class:"navbar-toggler",type:"button","data-toggle":"collapse","data-target":"#navbarNavAltMarkup","aria-controls":"navbarNavAltMarkup","aria-expanded":"false","aria-label":"Toggle navigation",children:(0,a.jsx)("span",{class:"navbar-toggler-icon"})}),(0,a.jsx)("div",{class:"collapse navbar-collapse",id:"navbarNavAltMarkup",children:(0,a.jsxs)("div",{class:"navbar-nav",children:[(0,a.jsx)("a",{class:"nav-item nav-link active",href:"/",children:"Home"}),(0,a.jsx)("a",{class:"nav-item nav-link active",href:"/slides",children:"Project Slideshow"}),(0,a.jsx)("a",{class:"nav-item nav-link",href:"/processbook",children:"Process Book"})]})})]})},7572:(e,t,o)=>{(window.__NEXT_P=window.__NEXT_P||[]).push(["/processbook",function(){return o(1102)}])}},e=>{var t=t=>e(e.s=t);e.O(0,[424,516,636,593,792],()=>t(7572)),_N_E=e.O()}]);